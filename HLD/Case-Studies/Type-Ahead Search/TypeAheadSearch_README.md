
# âœ¨ Type Ahead Search â€” Scalable Design with Personality

> "Type three letters. Get ten smart guesses. Keep your flow." â€” This README shows **how to build it right**.

A creative yet practical guide to designing a **Type Ahead Search** system that feels instant, scales to millions, and stays maintainable.

---

## ğŸ”¥ TL;DR
- **Goal**: Return **Topâ€‘10** relevant suggestions once a user types **â‰¥ 3 chars** (â‰¤ 20 chars).
- **Latency**: **p95 â‰¤ 100 ms** endâ€‘toâ€‘end.
- **Throughput**: ~**5,787 QPS** for ~**500M** requests/day (with debouncing).
- **Core**: Client **debounce â†’ API Gateway â†’ Suggestion Service â†’ Redis ZSET**. DB for cache misses. Streaming pipeline to keep indices fresh.

---

## ğŸ¯ Why This Matters
Typeâ€‘ahead reduces **timeâ€‘toâ€‘result**, improves **conversion**, and surfaces **discoverability**. Done right, it saves backend resources and delights users.

---

## âœ… Requirements
- Show suggestions for prefixes of length **3â€“20**.
- **Debounce 250â€“300 ms** on the client to avoid request storms.
- Responses must be **contextâ€‘relevant**, **languageâ€‘aware**, and **safe** (filters/blocklists).
- Separate **read** (suggest) and **write** (search log) paths for availability.

---

## ğŸ§­ UX Flow
1. User types: `whaâ€¦` â†’ client waits ~300 ms (debounce).
2. Client calls `GET /app/v1/suggestion?q=wha&lang=en`.
3. Service hits **Redis ZSET**: `ZRANGE SUG:en:wha 0 9`.
4. **Cache hit** â†’ return Topâ€‘10.
5. **Cache miss** â†’ read DB â†’ warm Redis â†’ return.
6. When user submits search, `POST /app/v1/search` emits an event for analytics & ranking.

---

## ğŸ“¡ APIs
### `GET /app/v1/suggestion`
Query params:
```json
{ "q": "wha", "lang": "en", "k": 10 }
```
Response:
```json
{ "status": "success", "suggestions": ["what is the color of sky", "what is 2+2"], "error": null }
```

### `POST /app/v1/search`
Body:
```json
{ "query": "what is the color of sky?", "lang": "en", "userId": "optional" }
```
Response:
```json
{ "status": "success", "updateCount": 1, "error": null }
```

---

## ğŸ§± Data Model
- **queries**: `{ query: stringâ‰¤100B, lang: string, total_count: int64, last_seen_ts: datetime }`
- **prefix_index**: `{ prefix: string, lang: string, topK: [ { query, score } ] }`
- **score** formula (example): `score = freq * time_decay + personalization_weight`
  - `time_decay` halfâ€‘life: **7â€“30 days** (choose per product cadence).

---

## ğŸ§® Sizing Math
- Daily suggestion calls: **â‰ˆ 500M** â†’ `500,000,000 / 86,400 â‰ˆ 5,787 QPS`.
- New queries/day: **â‰ˆ 50M** Ã— **~200 bytes** â‰ˆ **10 GB/day** â†’ **~3.6 TB/year** preâ€‘compaction.
- Cache footprint: if a prefix record â‰ˆ **200 B**, **16 GB Redis** holds **â‰ˆ80M** keys (excluding overhead).

---

## ğŸ—ï¸ Architecture (Visuals)
**PNG diagram for slides**:

![Architecture v2](architecture_typeahead_v2.png)

**Component Diagram**
[Component Diagram](image1.png)


**Sequence Diagram**
[Sequence Diagram](image2.png)

---

## ğŸ§° Redis Key Schema (Example)
- Key format: `SUG:{lang}:{prefix}`
- Operations:
```text
ZRANGE SUG:en:wha 0 9           # read Top-10
ZADD   SUG:en:wha score query   # warm/update
EXPIRE SUG:en:wha 86400         # optional TTL
```
- Avoid hot keys: **salt prefixes** for ultraâ€‘popular terms (e.g., `SUG:en:wha:#A`, `#B`) and aggregate.

---

## ğŸªµ Caching Strategy
- Prefer **ZSET** for ranked suggestions.
- **LRU + TTL** to control memory.
- **Proactive warming** after peak hours for popular prefixes.
- Track **hit ratio**; aim **> 90%** in steady state.

---

## ğŸ§­ Sharding & Indexing
- **Consistent hashing** across shards for `queries` and `prefix_index`.
- Regionâ€‘local replicas for reads; async crossâ€‘region replication.
- Batch rebuilds via stream pipeline to keep **Topâ€‘K per prefix** current.

---

## ğŸ§¨ Failure Modes & Mitigations
- Cache node down â†’ **graceful DB fallback**, reduce K (e.g., 10 â†’ 5), and lean hints.
- DB shard hot â†’ **split/salt popular prefixes**, backâ€‘pressure via gateway.
- Event bus lag â†’ **degrade to frequencyâ€‘only scoring** temporarily.

---

## ğŸ“ SLOs & Observability
- Availability â‰¥ **99%**.
- p95 latency â‰¤ **100 ms**.
- Metrics: cache hit ratio, stream lag, shard load, 4xx/5xx per endpoint.
- Tracing: client â†’ gateway â†’ service â†’ cache/DB; tag by **prefix length** & **region**.

---

## ğŸ” Privacy & Safety
- No raw PII in logs; if personalization is enabled, **hash/anonymize** IDs.
- Retention: **30â€“90 days** for raw; keep aggregates beyond.
- Blocklists + ML filters to prevent unsafe suggestions.

---

## ğŸ§ª Testing Playbook
- Load tests for p95/p99 latency at target QPS.
- Cache failure drills; DB fallback latency budget.
- Canary deploys; autoâ€‘rollback on SLO breach.

---

## ğŸš€ Implementation Notes
- Suggestion Service can be **stateless** (horizontal scaling).
- Use **gRPC** for internal calls if polyglot; HTTP/JSON for public API.
- Store queries **normalized** (lowercase, trimmed, punctuationâ€‘aware) with **language codes**.

---

## â“FAQ
**Q: Why not Trie only?** Tries are fast but memoryâ€‘heavy; ZSETs + materialized Topâ€‘K per prefix keep costs predictable.

**Q: How to personalize?** Blend global frequency with **user/topic embeddings** + recent interactions.

**Q: Multilingual?** Separate keys per language; add **transliteration** (e.g., Hinglish/Kannada â†” English) in the index builder.

---

### Attribution
Crafted for system design storytelling. Includes both **Mermaid diagrams** and a **PNG** for decks.

